{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheOneTrueGuy/202-Llama/blob/main/1Gemini_3cycle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o88Utuba8NiT"
      },
      "source": [
        "Digital daydreaming in latent space with CLIP Interogator and Stable Diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INL8Ds7z8dO5"
      },
      "outputs": [],
      "source": [
        "# reimplementing this to use google gemini API instead of clip interogator. Image generation will be handled with SD3-medium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git\n",
        "!pip install --upgrade accelerate #transformers\n",
        "!pip install omegaconf\n",
        "!apt-get install rsync # for better file transfer progress?\n",
        "!pip install noise\n",
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "9koGt_v0zsd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arj7-WnR_vCS"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "results_dir = '/content/drive/MyDrive/clip-loop'\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# fetch image_to_image.py\n",
        "#!git clone https://github.com/TheOneTrueGuy/DigitalDayDream.git\n",
        "#!cp DigitalDayDream/noiser.py /content/\n",
        "\n",
        "print(f\"\\nResults will be saved to {results_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install imagemagick\n",
        "!pip install opencv-python\n",
        "!cp /content/drive/MyDrive/stub_materials/broca1.txt /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/bio1.txt /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/command.txt /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/noiser.py /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/trim4.py /content/trim4.py\n",
        "!cp /content/drive/MyDrive/stub_materials/mirrorhalf.py /content/mirrorhalf.py\n",
        "!cp /content/drive/MyDrive/stub_materials/stereogram.py /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/scroll4.py /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/1julien3.sh /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/gauss_noiser.py /content/\n",
        "!cp /content/drive/MyDrive/stub_materials/moto*.txt /content/"
      ],
      "metadata": {
        "id": "aHowv7kF23_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title upload start image 512x512 will be quick and look good\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "name=uploaded.keys()\n",
        "name = list(name)[0]\n",
        "print(name)\n",
        "os.system(f\"convert {name} -resize 512x512! st.png\")"
      ],
      "metadata": {
        "id": "98OrVl6kjmfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(f\"convert st.jpg -resize 512x512! st.png\")"
      ],
      "metadata": {
        "id": "qhJe7pbymPPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4O2mYM6Kld4"
      },
      "outputs": [],
      "source": [
        "# run this first then make anim in next cell\n",
        "\n",
        "import torch\n",
        "#from diffusers import StableDiffusionXLImg2ImgPipeline\n",
        "#from diffusers import StableDiffusion3Pipeline\n",
        "from diffusers.models import AutoencoderKL\n",
        "\n",
        "#from diffusers import StableDiffusionImg2ImgPipeline\n",
        "from diffusers.utils import load_image\n",
        "from transformers import AutoTokenizer # for use with small sd\n",
        "#from diffusers import DiffusionPipeline\n",
        "import datetime as dt\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from PIL import ImageFont, ImageDraw, Image, ImageOps, ImageChops, ImageFilter\n",
        "\n",
        "class FakeSafety():\n",
        "    def __call__(self, clip_input, images):\n",
        "        return (images, [False])\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "# check to see if pipe is loaded, if not load it otherwise skip loading pipe\n",
        "\n",
        "#if pipe is None:\n",
        "\n",
        "#from google.colab import userdata\n",
        "#hf_auth=userdata.get('HF_TOKEN')\n",
        "#print(hf_auth)\n",
        "hf_auth=\"your HF auth here\"\n",
        "#!huggingface-cli login\n",
        "\n",
        "#from diffusers import StableDiffusion3Pipeline\n",
        "from diffusers import StableDiffusion3Img2ImgPipeline\n",
        "#pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
        "#                                                torch_dtype=torch.float16, use_auth_token=hf_auth)\n",
        "\n",
        "pipe = StableDiffusion3Img2ImgPipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
        "                                                torch_dtype=torch.float16, token=hf_auth,  text_encoder_3=None, tokenizer_3=None)\n",
        "\n",
        "#pipe = StableDiffusionImg2ImgPipeline.from_single_file(\n",
        "#  \"deliberatexp.safetensors\",\n",
        "#  torch_dtype=torch.float16,\n",
        "#)\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "#  \"runwayml/stable-diffusion-xl-refiner-1.0\", torch_dtype=torch.float16\n",
        "#) # can't use LoRa??\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "#  \"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16\n",
        "#) # can't use LoRa??\n",
        "\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"/content/drive/MyDrive/AI/models/stable-diffusion-xl-refiner-1.0/sd_xl_refiner_1.0.safetensors\",\n",
        "#  torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"/content/drive/MyDrive/AI/models/deliberate_v3.safetensors\",\n",
        "#  torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"sdxlUnstableDiffusers_v9DIVINITYMACHINEVAE.safetensors\",\n",
        "#  torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "#vae = AutoencoderKL.from_single_file(\"sdxl_vae.safetensors\")\n",
        "\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "#  \"segmind/small-sd\", torch_dtype=torch.float16\n",
        "#) # error: 'NoneType' object has no attribute 'tokenize'\n",
        "#I get it, need to supply a tokenizer before calling \"generate, cool\"\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "#pipe.tokenizer=tokenizer\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "#  \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16\n",
        "#)\n",
        "# use this one for loras\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"sdxlUnstableDiffusers_v9DIVINITYMACHINEVAE.safetensors\", torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "# sdxlUnstableDiffusers_v9DIVINITYMACHINEVAE.safetensors\n",
        "#cyborg\n",
        "#chillpixel/starlight-animated-sdxl\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "#  \"\", torch_dtype=torch.float16\n",
        "#)\n",
        "# stabilityai/stable-diffusion-xl-base-1.0 # use with load lora\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
        "#  \"/content/drive/MyDrive/AI/models/deliberate_v3.safetensors\", torch_dtype=torch.float16, repo_type=\"local\"\n",
        "#)\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\"/content/drive/MyDrive/AI/models/deliberate_v3.safetensors\")\n",
        "\n",
        "\n",
        "#vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"sd_xl_refiner_1.0.safetensors\", vae=vae,\n",
        "#  torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "\n",
        "#text_encoder = CLIPTextModel.from_pretrained('clip_g.safetensors')\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(<checkpoint path>, text_encoder=text_encoder)\n",
        "\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"sd3_medium.safetensors\", #text_encoder=text_encoder, #vae=vae,\n",
        "#  torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "\n",
        "#pipe = StableDiffusionXLImg2ImgPipeline.from_single_file(\n",
        "#  \"sd3_medium_incl_clips.safetensors\", # text_encoder=text_encoder, #vae=vae,\n",
        "#  torch_dtype=torch.float16, use_safetensors=True, repo_type=\"local\"\n",
        "#)\n",
        "\n",
        "#from diffusers import EDMDPMSolverMultistepScheduler\n",
        "#pipe.scheduler = EDMDPMSolverMultistepScheduler()\n",
        "\n",
        "#pipe = DiffusionPipeline.from_pretrained(\n",
        "#            \"playgroundai/playground-v2.5-1024px-aesthetic\",\n",
        "#            torch_dtype=torch.float16,\n",
        "#            variant=\"fp16\",\n",
        "#        ).to(\"cuda\")\n",
        "\n",
        "      # # Optional: Use DPM++ 2M Karras scheduler for crisper fine details\n",
        "      # from diffusers import EDMDPMSolverMultistepScheduler\n",
        "      # pipe.scheduler = EDMDPMSolverMultistepScheduler()\n",
        "\n",
        "pipe = pipe.to(\"cuda\")\n",
        "pipe.safety_checker = FakeSafety()\n",
        "\n",
        "# time for local loading of model: 2:30\n",
        "# time for remote loading of model: 1m 0s\n",
        "\n",
        "def generate_phrase(word_string, spacing, added_word):\n",
        "    words = word_string.split()\n",
        "    result = []\n",
        "    lenwo = len(words)\n",
        "\n",
        "    for yn in range(lenwo):\n",
        "        result.append(words[(yn) % lenwo])\n",
        "        if yn % spacing == spacing - 1:\n",
        "            result.append(added_word)\n",
        "\n",
        "    phrase = ' '.join(result)\n",
        "    return phrase\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class Stack:\n",
        "    def __init__(self):\n",
        "        self.stack = deque(maxlen=9)\n",
        "        self.count = 0\n",
        "\n",
        "    def push(self, string1, string2, string3):\n",
        "        self.stack.append([string1, string2, string3])\n",
        "        self.count += 1\n",
        "        if self.count>19: self.count=19\n",
        "\n",
        "    def get_stack(self):\n",
        "        return list(self.stack)\n",
        "\n",
        "    def get_item(self, index):\n",
        "        if index >= 0 and index < len(self.stack):\n",
        "          return self.stack[index]\n",
        "        else:\n",
        "          return [\"error\", \"error\", \"error\"]\n",
        "\n",
        "stacky=Stack()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
      ],
      "metadata": {
        "id": "cW63xMcnmYYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vin6RgA85gT",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@title Create animation!\n",
        "\n",
        "start_image_path = \"/content/st.png\" #@param {type:\"string\"}\n",
        "style_prompt = \"experimental medium, asmr, \" #@param {type:\"string\"}\n",
        "dir_name = \"Gemini_cycle_2\" #@param {type:\"string\"}\n",
        "max_frames = 800 #@param {type:\"integer\"}\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import random\n",
        "from transformers import CLIPTokenizer\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\") # need to fix for correct token length but sorta works\n",
        "\n",
        "#had to remove motion control for github listing\n",
        "\n",
        "#file2 = open(\"moto1g.txt\")\n",
        "#lines2 = file2.read().replace(\"\\n\", \"\")\n",
        "#file2.close()\n",
        "## use at least one following lines if you don't have any moto files and comment out above 3 lines\n",
        "#lines2=\"zi zr zi zr zi zr zi sr zt sl zi sr zi sl zi sr zi sl zi sr zi sl sl sl sr sr sr zi zr zi sd sd zr sd sd nm su su su\"\n",
        "#lines2=\" \".join([\"zi\"] * 22) + \" \" + \" \".join([\"tr\"] * 15) +  \" \" + \" \".join([\"nm\"] * 9) #loopmoto\n",
        "#lines2=lines2+\" \".join([\"zi\"] * 20) + \" \" + \" \".join([\"td\"] * 15) +  \" \" + \" \".join([\"zi\"] * 22) +  \" \" + \" \".join([\"nm\"] * 12)\n",
        "#lines2=\" \".join([\"zi\"] * 18) + \" \" + \" \".join([\"zr\"] * 5) + \" \" + \" \".join([\"nm\"] * 20) + \" \" + \" \".join([\"zr\"] * 6) #loopmoto\n",
        "#lines2=\" \".join([\"tu\"] * 30) + \" \" + \" \".join([\"tl\"] * 30) + \" \" + \" \".join([\"zi\"] * 50) + \" \".join([\"td\"] * 30) + \" \" + \" \".join([\"tr\" ] * 30) + \" \" + \" \".join([\"zi\"] * 50) #loopmoto\n",
        "moto=lines2.split(\" \")\n",
        "lenmo=len(moto)\n",
        "\n",
        "nois=0.8\n",
        "\n",
        "rond=random.randint(1, 10000000)\n",
        "os.makedirs(dir_name, exist_ok=True)\n",
        "dialog = open(\"dialog.txt\", \"a\", encoding=\"utf-8\")\n",
        "dialog.write(\"random seed:\"+str(rond) + \"\\n\")\n",
        "primary=\"This is a visual representation of your current thinking. Using only visually descriptive terms describe the next thought that occurs to you based on it.\"\n",
        "for xn in range(0, max_frames, 1):\n",
        "  im = Image.open(start_image_path if xn == 0 else \"st.png\").convert(\"RGB\")\n",
        "  im = im.resize((512,512))\n",
        "\n",
        "  buffered = BytesIO()\n",
        "  im.save(buffered, format=\"PNG\")\n",
        "  img_str = base64.b64encode(buffered.getvalue()).decode()\n",
        "  #cliprompt = ci.interrogate_classic(im)\n",
        "  #numtokens=len(tokenizer.tokenize(cliprompt))\n",
        "  # Pass the base64 encoded image to the model\n",
        "  gemprompt=model.generate_content([primary, {\"image\": {\"data\": img_str}}])\n",
        "\n",
        "\n",
        "  # Check if the response contains any parts\n",
        "  if gemprompt.parts:\n",
        "    if style_prompt: #replace this with output from gemini vision model\n",
        "        cliprompt = gemprompt.text # Access the text content of the response\n",
        "        cliprompt = style_prompt + cliprompt\n",
        "    else:\n",
        "        cliprompt = gemprompt.text #sometimes I add unnecessary steps just for the hell of it.\n",
        "\n",
        "    gemprompt = cliprompt\n",
        "  else:\n",
        "    # Handle the case where the response is blocked\n",
        "    print(f\"{xn:04d}: Response blocked by safety filter\\n\")\n",
        "    dialog.write(f\"{xn:04d}: Response blocked by safety filter\\n\")\n",
        "    continue # Skip to the next iteration\n",
        "\n",
        "  # how to shorten gemprompt length in tokens so they are less than 77?\n",
        "  gem_token_len = len(tokenizer.tokenize(gemprompt))\n",
        "  if gem_token_len > 70: # extra 7 just to be sure\n",
        "    gemprompt = tokenizer.decode(tokenizer.encode(gemprompt)[:70])\n",
        "\n",
        "\n",
        "  print(f\"{xn:04d}: {gemprompt}\\n\")\n",
        "  #dialog.write(f\"{xn:04d}: {cliprompt},\\n\")\n",
        "  dialog.write(f\"{xn:04d}: {gemprompt},\\n\")\n",
        "\n",
        "  image = pipe(\n",
        "      prompt=gemprompt,\n",
        "      image=im,\n",
        "      strength=0.6,\n",
        "      guidance_scale=8.2,\n",
        "      num_inference_steps=16\n",
        "      ).images[0]\n",
        "\n",
        "  image.save(\"st.png\")\n",
        "  #os.system(\"python noiser.py -p prev.png -n \" +str(int(xn*2.8)))\n",
        "  image.save(f\"{dir_name}/{xn:04d}.png\")\n",
        "\n",
        "  # and you will have to compile the video with ffmpeg or something\n",
        "  #next 2 lines apply motion distortifrom moto*.txt motion fileson\n",
        "\n",
        "  print(str(xn) + \" of \" + str(max_frames) + \" motion:\" + moto[xn%lenmo] )\n",
        "\n",
        "  #coman=\"bash 1julien3.sh st.png \" + moto[xn%lenmo]\n",
        "\n",
        "  #would like output from this bash command captured and printed out\n",
        "  #os.system(coman)\n",
        "\n",
        "\n",
        "  # be sure to add extra noise if zooming out i.e. moto1f\n",
        "\n",
        "  #os.system(f\"python gauss_noiser.py st.png {nois} st.png\")\n",
        "\n",
        "dialog.close()\n",
        "\n",
        "# zip animation frames and put in Google Drive\n",
        "year_month_day = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "zip_name = f\"{year_month_day}_{dir_name}.zip\"\n",
        "\n",
        "!mv dialog.txt $dir_name/\n",
        "!zip -r $zip_name $dir_name/\n",
        "!cp $zip_name $results_dir\n",
        "\n",
        "print(f\"Animation frames exported to {results_dir}/{zip_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " painting : photorealistic image rendered giza : Uxmal  desert, : jungle,  painting : rendered image  drawing : black and white photo : colorful stucco  egyptian men : elegant parrots on, : on surrounded by techno jungle,  dreadlocks : feathers skull:skully skull:white turnip apples:crystals green:green and red fractal:neon liquid paint splash psychedelic:asymmetric fractal kaleidoscop spiral:maze puzzle\n",
        " woman:flower headphones on:mechanical tendrils a close up : wide angle a close up of a person wearing headphones:wide angle view of a factory"
      ],
      "metadata": {
        "id": "h0jxlEw2EAi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1QN-3NUFVtz"
      },
      "outputs": [],
      "source": [
        "#run this cell after starting cell above if you don't want to burn\n",
        "#google compute credits unnecessarily.\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year_month_day = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
        "zip_name = f\"{year_month_day}_{dir_name}.zip\"\n",
        "!mv dialog.txt $dir_name/\n",
        "!zip -r $zip_name $dir_name/\n",
        "!cp $zip_name $results_dir\n"
      ],
      "metadata": {
        "id": "INXZDUG5KepF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#working scrap section, sorry\n",
        "file2 = open(\"moto1.txt\")\n",
        "lines2 = file2.read().replace(\"\\n\", \" \")\n",
        "file2.close()\n",
        "moto=lines2.split(\" \")\n",
        "lenmo=len(moto)"
      ],
      "metadata": {
        "id": "iS3Na3rWdtBa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a226ec2bc5421555d34070094e17dd9cf5fa466c5c99c238543997899dd52977"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}